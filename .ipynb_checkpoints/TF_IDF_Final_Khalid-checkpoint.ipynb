{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b0078",
   "metadata": {},
   "source": [
    "### Preprocess steps: \n",
    "Firstly the comments are being preprocessed here through tokenizing and PoS tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f64c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Meghna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('Labelled_Dataset.csv')\n",
    "\n",
    "# Extract the comments from the dataset\n",
    "comments = df['COMMENTS'].tolist()\n",
    "\n",
    "# Tokenize the comments\n",
    "tokenized_comments = [nltk.word_tokenize(comment) for comment in comments]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform part of speech tagging on the tokenized comments\n",
    "pos_tagged_comments = [nltk.pos_tag(comment) for comment in tokenized_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340b7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the PoS tagging\n",
    "#print(pos_tagged_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c166b0",
   "metadata": {},
   "source": [
    "### Removing Emoji:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b05ec8",
   "metadata": {},
   "source": [
    "A function is defined for removing emoji, emoticons from the text and it is applied to the comments column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f063eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoticons(text):\n",
    "  # Remove all emojis and emoticons from the text\n",
    "  return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Remove emojis and emoticons from the 'COMMENTS' column of the dataframe\n",
    "df['COMMENTS'] = df['COMMENTS'].apply(remove_emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f4a64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>COMMENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Im back and unimpressed The film is the second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>What timeline did a jump to I remember waiting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The worst part of this are the dinosaurs thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Baby blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>WOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This movie was amazing It was well thought out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>going watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>All Jurassic Park movies belong to Sam Neil No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Funny you dont see any Mammoths in these films...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Those raptor sounds are really getting old Jee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL                                            COMMENTS\n",
       "0  Negative   Im back and unimpressed The film is the second...\n",
       "1   Neutral   What timeline did a jump to I remember waiting...\n",
       "2  Negative   The worst part of this are the dinosaurs thriv...\n",
       "3   Neutral                                           Baby blue\n",
       "4  Positive                                                 WOW\n",
       "5  Positive   This movie was amazing It was well thought out...\n",
       "6   Neutral                                         going watch\n",
       "7   Neutral   All Jurassic Park movies belong to Sam Neil No...\n",
       "8   Neutral   Funny you dont see any Mammoths in these films...\n",
       "9  Negative   Those raptor sounds are really getting old Jee..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10) # This is to check the top ten data list of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eff1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "#df = pd.read_csv('/Users/rising.volkan007/Desktop/sentiment analysis/Dataset-rebalanced.csv')\n",
    "\n",
    "# Tokenize the comments\n",
    "df['TOKENIZED_COMMENTS'] = df['COMMENTS'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667e23f",
   "metadata": {},
   "source": [
    "#### Now function for preprocess_comment and preprocess_text will be defined so that it can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50068c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'comment']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    \n",
    "    # Tokenize the comment\n",
    "    words = word_tokenize(comment)\n",
    "    \n",
    "    return words\n",
    "\n",
    "# call the function and store the result in variable\n",
    "result = preprocess_comment(\"This is an example comment\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929b2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (3, 2)\t1\n",
      "  (4, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    comment = text\n",
    "    # Tokenize the comment\n",
    "    words = word_tokenize(comment)\n",
    "\n",
    "    # Fit the vectorizer on the words\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit_transform(words)\n",
    "    return vectors\n",
    "\n",
    "# call the function\n",
    "vectors = preprocess_text(\"This is an example comment\")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e71949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    comment = text\n",
    "    # Tokenize the comment\n",
    "    words = word_tokenize(comment)\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the lemmatized words\n",
    "    vectors = vectorizer.fit_transform(lemmatized_words)\n",
    "\n",
    "    # View the vectors as a dense array\n",
    "    return vectors.toarray()\n",
    "\n",
    "# call the function\n",
    "vectors = preprocess_text(\"This is an example comment\")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a924e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LABEL ', 'COMMENTS', 'TOKENIZED_COMMENTS'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4fe3e",
   "metadata": {},
   "source": [
    "LABEL column seems to have an extra space which needs to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d10b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'LABEL ']\n",
    "df.rename(columns={'LABEL ': 'LABEL'}, inplace=True) # LABEL column is renamed removing that extra white space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7cc39",
   "metadata": {},
   "source": [
    "Here the Labels from the LABEL column are being extracted and through strip() function any duplicate and white spaced values are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479a41a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199,)\n"
     ]
    }
   ],
   "source": [
    "# Extract the labels from the dataset\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "# Add the labels as a new column in the dataframe\n",
    "labels = [label.strip() for label in labels]\n",
    "\n",
    "df['LABEL'] = labels\n",
    "\n",
    "# Check the shape of the 'LABEL' column\n",
    "print(df['LABEL'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c80770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative    512\n",
      "Positive    511\n",
      "Neutral     176\n",
      "Name: LABEL, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAHNCAYAAAA0bIApAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoPElEQVR4nO3df3RU5Z3H8c+QkCEJSSBEMowmmmiwiwkWgyJpFwIEEEHKwbMsQqtVdFEQjIBRyp4VbZtQTgkRadmqrETYCN0DqVJ/LFAxWxpZIfKbRWyNEiTTCMRJIDGBcPePHuZ0CKCjIfdJ7vt1zvwx9z7B79hr8+bOzL0uy7IsAQAAGKSL3QMAAABciEABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHHC7R7gmzh37pyOHTummJgYuVwuu8cBAABfg2VZqq+vl9frVZculz9H0iED5dixY0pKSrJ7DAAA8A1UVVXpmmuuueyaDhkoMTExkv72AmNjY22eBgAAfB11dXVKSkoK/B6/nA4ZKOff1omNjSVQAADoYL7OxzP4kCwAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOOE2z1AZ3fdU2/YPUKn8MmisXaP0GlwTLYdjkvgyuEMCgAAMA6BAgAAjEOgAAAA44QUKAsXLpTL5Qp6eDyewH7LsrRw4UJ5vV5FRkYqOztbBw4cCPozmpqaNGvWLCUkJCg6Olrjx4/X0aNH2+bVAACATiHkMyg33XSTqqurA499+/YF9i1evFiFhYVavny5duzYIY/Ho5EjR6q+vj6wJjc3V6WlpVq7dq22bdumU6dOady4cWppaWmbVwQAADq8kL/FEx4eHnTW5DzLslRUVKQFCxZo4sSJkqTi4mIlJiaqpKRE06dPl9/v18qVK7V69Wrl5ORIktasWaOkpCRt2bJFo0eP/pYvBwAAdAYhn0H56KOP5PV6lZKSosmTJ+vjjz+WJFVWVsrn82nUqFGBtW63W0OHDlV5ebkkqaKiQmfOnAla4/V6lZ6eHlhzMU1NTaqrqwt6AACAziukQBk0aJBeeeUV/fd//7defPFF+Xw+ZWVl6cSJE/L5fJKkxMTEoJ9JTEwM7PP5fIqIiFDPnj0vueZiCgoKFBcXF3gkJSWFMjYAAOhgQgqUMWPG6O6771ZGRoZycnL0xht/u+BTcXFxYI3L5Qr6GcuyWm270FetmT9/vvx+f+BRVVUVytgAAKCD+VZfM46OjlZGRoY++uijwOdSLjwTUlNTEzir4vF41NzcrNra2kuuuRi3263Y2NigBwAA6Ly+VaA0NTXp//7v/9SnTx+lpKTI4/Fo8+bNgf3Nzc0qKytTVlaWJCkzM1Ndu3YNWlNdXa39+/cH1gAAAIT0LZ558+bprrvuUnJysmpqavSzn/1MdXV1uu++++RyuZSbm6v8/HylpaUpLS1N+fn5ioqK0pQpUyRJcXFxmjZtmubOnatevXopPj5e8+bNC7xlBAAAIIUYKEePHtU999yj48eP66qrrtLtt9+u7du369prr5Uk5eXlqbGxUTNmzFBtba0GDRqkTZs2KSYmJvBnLF26VOHh4Zo0aZIaGxs1YsQIrVq1SmFhYW37ygAAQIflsizLsnuIUNXV1SkuLk5+v9/4z6Nw59i2wV1j2w7HZNvhuARCE8rvb+7FAwAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAON8q0ApKCiQy+VSbm5uYJtlWVq4cKG8Xq8iIyOVnZ2tAwcOBP1cU1OTZs2apYSEBEVHR2v8+PE6evTotxkFAAB0It84UHbs2KEXXnhB/fv3D9q+ePFiFRYWavny5dqxY4c8Ho9Gjhyp+vr6wJrc3FyVlpZq7dq12rZtm06dOqVx48appaXlm78SAADQaXyjQDl16pSmTp2qF198UT179gxstyxLRUVFWrBggSZOnKj09HQVFxeroaFBJSUlkiS/36+VK1dqyZIlysnJ0YABA7RmzRrt27dPW7ZsaZtXBQAAOrRvFCgzZ87U2LFjlZOTE7S9srJSPp9Po0aNCmxzu90aOnSoysvLJUkVFRU6c+ZM0Bqv16v09PTAmgs1NTWprq4u6AEAADqv8FB/YO3atfrggw+0Y8eOVvt8Pp8kKTExMWh7YmKiPv3008CaiIiIoDMv59ec//kLFRQU6Jlnngl1VAAA0EGFdAalqqpKjz32mNasWaNu3bpdcp3L5Qp6bllWq20Xutya+fPny+/3Bx5VVVWhjA0AADqYkAKloqJCNTU1yszMVHh4uMLDw1VWVqZly5YpPDw8cObkwjMhNTU1gX0ej0fNzc2qra295JoLud1uxcbGBj0AAEDnFVKgjBgxQvv27dPu3bsDj4EDB2rq1KnavXu3UlNT5fF4tHnz5sDPNDc3q6ysTFlZWZKkzMxMde3aNWhNdXW19u/fH1gDAACcLaTPoMTExCg9PT1oW3R0tHr16hXYnpubq/z8fKWlpSktLU35+fmKiorSlClTJElxcXGaNm2a5s6dq169eik+Pl7z5s1TRkZGqw/dAgAAZwr5Q7JfJS8vT42NjZoxY4Zqa2s1aNAgbdq0STExMYE1S5cuVXh4uCZNmqTGxkaNGDFCq1atUlhYWFuPAwAAOiCXZVmW3UOEqq6uTnFxcfL7/cZ/HuW6p96we4RO4ZNFY+0eodPgmGw7HJdAaEL5/c29eAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGCckAJlxYoV6t+/v2JjYxUbG6vBgwfrrbfeCuy3LEsLFy6U1+tVZGSksrOzdeDAgaA/o6mpSbNmzVJCQoKio6M1fvx4HT16tG1eDQAA6BRCCpRrrrlGixYt0s6dO7Vz504NHz5cP/jBDwIRsnjxYhUWFmr58uXasWOHPB6PRo4cqfr6+sCfkZubq9LSUq1du1bbtm3TqVOnNG7cOLW0tLTtKwMAAB1WSIFy11136c4771Tfvn3Vt29f/fznP1f37t21fft2WZaloqIiLViwQBMnTlR6erqKi4vV0NCgkpISSZLf79fKlSu1ZMkS5eTkaMCAAVqzZo327dunLVu2XJEXCAAAOp5v/BmUlpYWrV27VqdPn9bgwYNVWVkpn8+nUaNGBda43W4NHTpU5eXlkqSKigqdOXMmaI3X61V6enpgzcU0NTWprq4u6AEAADqvkANl37596t69u9xutx5++GGVlpaqX79+8vl8kqTExMSg9YmJiYF9Pp9PERER6tmz5yXXXExBQYHi4uICj6SkpFDHBgAAHUjIgXLjjTdq9+7d2r59ux555BHdd999OnjwYGC/y+UKWm9ZVqttF/qqNfPnz5ff7w88qqqqQh0bAAB0ICEHSkREhG644QYNHDhQBQUFuvnmm/Xcc8/J4/FIUqszITU1NYGzKh6PR83Nzaqtrb3kmotxu92Bbw6dfwAAgM7rW18HxbIsNTU1KSUlRR6PR5s3bw7sa25uVllZmbKysiRJmZmZ6tq1a9Ca6upq7d+/P7AGAAAgPJTFP/nJTzRmzBglJSWpvr5ea9eu1bvvvqu3335bLpdLubm5ys/PV1pamtLS0pSfn6+oqChNmTJFkhQXF6dp06Zp7ty56tWrl+Lj4zVv3jxlZGQoJyfnirxAAADQ8YQUKH/961/1ox/9SNXV1YqLi1P//v319ttva+TIkZKkvLw8NTY2asaMGaqtrdWgQYO0adMmxcTEBP6MpUuXKjw8XJMmTVJjY6NGjBihVatWKSwsrG1fGQAA6LBclmVZdg8Rqrq6OsXFxcnv9xv/eZTrnnrD7hE6hU8WjbV7hE6DY7LtcFwCoQnl9zf34gEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxQgqUgoIC3XrrrYqJiVHv3r01YcIEffjhh0FrLMvSwoUL5fV6FRkZqezsbB04cCBoTVNTk2bNmqWEhARFR0dr/PjxOnr06Ld/NQAAoFMIKVDKyso0c+ZMbd++XZs3b9bZs2c1atQonT59OrBm8eLFKiws1PLly7Vjxw55PB6NHDlS9fX1gTW5ubkqLS3V2rVrtW3bNp06dUrjxo1TS0tL270yAADQYYWHsvjtt98Oev7yyy+rd+/eqqio0JAhQ2RZloqKirRgwQJNnDhRklRcXKzExESVlJRo+vTp8vv9WrlypVavXq2cnBxJ0po1a5SUlKQtW7Zo9OjRrf65TU1NampqCjyvq6sL+YUCAICO41t9BsXv90uS4uPjJUmVlZXy+XwaNWpUYI3b7dbQoUNVXl4uSaqoqNCZM2eC1ni9XqWnpwfWXKigoEBxcXGBR1JS0rcZGwAAGO4bB4plWZozZ46+//3vKz09XZLk8/kkSYmJiUFrExMTA/t8Pp8iIiLUs2fPS6650Pz58+X3+wOPqqqqbzo2AADoAEJ6i+fvPfroo9q7d6+2bdvWap/L5Qp6bllWq20Xutwat9stt9v9TUcFAAAdzDc6gzJr1iy9/vrr2rp1q6655prAdo/HI0mtzoTU1NQEzqp4PB41Nzertrb2kmsAAICzhRQolmXp0Ucf1YYNG/TOO+8oJSUlaH9KSoo8Ho82b94c2Nbc3KyysjJlZWVJkjIzM9W1a9egNdXV1dq/f39gDQAAcLaQ3uKZOXOmSkpK9NprrykmJiZwpiQuLk6RkZFyuVzKzc1Vfn6+0tLSlJaWpvz8fEVFRWnKlCmBtdOmTdPcuXPVq1cvxcfHa968ecrIyAh8qwcAADhbSIGyYsUKSVJ2dnbQ9pdfflk//vGPJUl5eXlqbGzUjBkzVFtbq0GDBmnTpk2KiYkJrF+6dKnCw8M1adIkNTY2asSIEVq1apXCwsK+3asBAACdgsuyLMvuIUJVV1enuLg4+f1+xcbG2j3OZV331Bt2j9ApfLJorN0jdBock22H4xIITSi/v7kXDwAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADBOSJe6BwCgrXF147bTma5uzBkUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxgk5UP7nf/5Hd911l7xer1wul373u98F7bcsSwsXLpTX61VkZKSys7N14MCBoDVNTU2aNWuWEhISFB0drfHjx+vo0aPf6oUAAIDOI+RAOX36tG6++WYtX778ovsXL16swsJCLV++XDt27JDH49HIkSNVX18fWJObm6vS0lKtXbtW27Zt06lTpzRu3Di1tLR881cCAAA6jfBQf2DMmDEaM2bMRfdZlqWioiItWLBAEydOlCQVFxcrMTFRJSUlmj59uvx+v1auXKnVq1crJydHkrRmzRolJSVpy5YtGj169Ld4OQAAoDNo08+gVFZWyufzadSoUYFtbrdbQ4cOVXl5uSSpoqJCZ86cCVrj9XqVnp4eWHOhpqYm1dXVBT0AAEDn1aaB4vP5JEmJiYlB2xMTEwP7fD6fIiIi1LNnz0uuuVBBQYHi4uICj6SkpLYcGwAAGOaKfIvH5XIFPbcsq9W2C11uzfz58+X3+wOPqqqqNpsVAACYp00DxePxSFKrMyE1NTWBsyoej0fNzc2qra295JoLud1uxcbGBj0AAEDn1aaBkpKSIo/Ho82bNwe2NTc3q6ysTFlZWZKkzMxMde3aNWhNdXW19u/fH1gDAACcLeRv8Zw6dUp//vOfA88rKyu1e/duxcfHKzk5Wbm5ucrPz1daWprS0tKUn5+vqKgoTZkyRZIUFxenadOmae7cuerVq5fi4+M1b948ZWRkBL7VAwAAnC3kQNm5c6eGDRsWeD5nzhxJ0n333adVq1YpLy9PjY2NmjFjhmprazVo0CBt2rRJMTExgZ9ZunSpwsPDNWnSJDU2NmrEiBFatWqVwsLC2uAlAQCAji7kQMnOzpZlWZfc73K5tHDhQi1cuPCSa7p166bnn39ezz//fKj/eAAA4ADciwcAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGsTVQfv3rXyslJUXdunVTZmam/vjHP9o5DgAAMIRtgbJu3Trl5uZqwYIF2rVrl/7xH/9RY8aM0ZEjR+waCQAAGMK2QCksLNS0adP04IMP6h/+4R9UVFSkpKQkrVixwq6RAACAIcLt+Ic2NzeroqJCTz31VND2UaNGqby8vNX6pqYmNTU1BZ77/X5JUl1d3ZUdtA2ca2qwe4ROoSP8b91RcEy2HY7LtsEx2XZMPybPz2dZ1leutSVQjh8/rpaWFiUmJgZtT0xMlM/na7W+oKBAzzzzTKvtSUlJV2xGmCWuyO4JgNY4LmGajnJM1tfXKy4u7rJrbAmU81wuV9Bzy7JabZOk+fPna86cOYHn586d08mTJ9WrV6+LrsfXV1dXp6SkJFVVVSk2NtbucQCOSRiJ47JtWJal+vp6eb3er1xrS6AkJCQoLCys1dmSmpqaVmdVJMntdsvtdgdt69Gjx5Uc0XFiY2P5jw5G4ZiEiTguv72vOnNyni0fko2IiFBmZqY2b94ctH3z5s3KysqyYyQAAGAQ297imTNnjn70ox9p4MCBGjx4sF544QUdOXJEDz/8sF0jAQAAQ9gWKP/8z/+sEydO6Nlnn1V1dbXS09P15ptv6tprr7VrJEdyu916+umnW72FBtiFYxIm4rhsfy7r63zXBwAAoB1xLx4AAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQHOqPf/yjfvjDH2rw4MH67LPPJEmrV6/Wtm3bbJ4MTrZ69Wp973vfk9fr1aeffipJKioq0muvvWbzZHCKurq6r/3AlUWgOND69es1evRoRUZGateuXWpqapL0t9tf5+fn2zwdnGrFihWaM2eO7rzzTn3xxRdqaWmR9LcbgxYVFdk7HByjR48e6tmz52Uf59fgyuJKsg40YMAAPf7447r33nsVExOjPXv2KDU1Vbt379Ydd9zR6i7TQHvo16+f8vPzNWHChKDjcv/+/crOztbx48ftHhEOUFZW9rXXDh069ApOAtvuxQP7fPjhhxoyZEir7bGxsfriiy/afyBAUmVlpQYMGNBqu9vt1unTp22YCE5EdJiDQHGgPn366M9//rOuu+66oO3btm1TamqqPUPB8VJSUrR79+5WNwx966231K9fP5umAqSGhgYdOXJEzc3NQdv79+9v00TOQKA40PTp0/XYY4/pP/7jP+RyuXTs2DG99957mjdvnv7t3/7N7vHgUE888YRmzpypL7/8UpZl6f3339err76qgoICvfTSS3aPBwf6/PPPdf/99+utt9666P7zn5PClUGgOFBeXp78fr+GDRumL7/8UkOGDJHb7da8efP06KOP2j0eHOr+++/X2bNnlZeXp4aGBk2ZMkVXX321nnvuOU2ePNnu8eBAubm5qq2t1fbt2zVs2DCVlpbqr3/9q372s59pyZIldo/X6fEhWQdraGjQwYMHde7cOfXr10/du3e3eyRAknT8+HGdO3dOvXv3tnsUOFifPn302muv6bbbblNsbKx27typvn376vXXX9fixYu5LMMVxteMHai4uFinT59WVFSUBg4cqNtuu404ge2eeeYZ/eUvf5EkJSQkECew3enTpwPHYXx8vD7//HNJUkZGhj744AM7R3MEAsWB5s2bp969e2vy5Mn6/e9/r7Nnz9o9EqD169erb9++uv3227V8+fLALwPALjfeeKM+/PBDSdJ3v/td/eY3v9Fnn32mf//3f1efPn1snq7zI1AcqLq6WuvWrVNYWJgmT56sPn36aMaMGSovL7d7NDjY3r17tXfvXg0fPlyFhYW6+uqrdeedd6qkpEQNDQ12jwcHys3NVXV1tSTp6aef1ttvv63k5GQtW7aMi1q2Az6D4nANDQ0qLS1VSUmJtmzZomuuuSZwmh2w05/+9CeVlJTov/7rv/Tll19yaXHYrqGhQYcOHVJycrISEhLsHqfT4wyKw0VFRWn06NEaM2aM0tLS9Mknn9g9EiBJio6OVmRkpCIiInTmzBm7x4HDnDlzRqmpqTp48GBgW1RUlG655RbipJ0QKA7V0NCg//zP/9Sdd94pr9erpUuXasKECdq/f7/do8HBKisr9fOf/1z9+vXTwIED9cEHH2jhwoXcfgHtrmvXrmpqapLL5bJ7FMfiLR4Huueee7Rx40ZFRUXpn/7pnzR16lRlZWXZPRYcbvDgwXr//feVkZGhqVOnBq6DAthl0aJFOnTokF566SWFh3PZsPbGv3EHcrlcWrdunUaPHs1/dDDGsGHD9NJLL+mmm26yexRAkvS///u/+sMf/qBNmzYpIyND0dHRQfs3bNhg02TOwBkUAAAu4v7777/s/pdffrmdJnEmAsUhli1bpn/5l39Rt27dtGzZssuunT17djtNBaebM2eOfvrTnyo6Olpz5sy57NrCwsJ2mgqACQgUh0hJSdHOnTvVq1cvpaSkXHKdy+XSxx9/3I6TwcnO39+kR48eGjZs2GXXbt26tZ2mAv5m+PDh2rBhg3r06BG0va6uThMmTNA777xjz2AOQaAAAHARXbp0kc/na3XbhZqaGl199dV8/f0K42vGDvTss89e9MqcjY2NevbZZ22YCJAeeOAB1dfXt9p++vRpPfDAAzZMBKc6f1VjSTp48GDg+d69e7Vr1y6tXLmSb5i1A86gOFBYWJiqq6tb/a3gxIkT6t27t1paWmyaDE52qePy+PHj8ng83DMK7aZLly6B659c7FdkZGSknn/+ecL5CuM7pg5kWdZFLz60Z88excfH2zARnKyurk6WZcmyLNXX16tbt26BfS0tLXrzzTe5szHaVWVlpSzLUmpqqt5//31dddVVgX0RERHq3bu3wsLCbJzQGQgUB+nZs6dcLpdcLpf69u0bFCktLS06deqUHn74YRsnhBP16NEj6Li8kMvl0jPPPGPDZHCqa6+9VpJ07tw5mydxNt7icZDi4mJZlqUHHnhARUVFiouLC+yLiIjQddddp8GDB9s4IZyorKxMlmVp+PDhWr9+fdBZvIiICF177bXyer02TgineuWVVy67/957722nSZyJQHGgsrIyZWVlqWvXrnaPAgR8+umnSk5O5t4nMEbPnj2Dnp85c0YNDQ2KiIhQVFSUTp48adNkzkCgOFxjY2Orr8rFxsbaNA2cZu/evUpPT1eXLl0C35q4lP79+7fTVMClffTRR3rkkUf0xBNPaPTo0XaP06kRKA7U0NCgvLw8/fa3v9WJEyda7edbPGgvf3+difPfnLjY/yW5XC6OSxhj586d+uEPf6hDhw7ZPUqnxodkHeiJJ57Q1q1b9etf/1r33nuvfvWrX+mzzz7Tb37zGy1atMju8eAglZWVgW9IVFZW2jwN8PWEhYXp2LFjdo/R6XEGxYGSk5P1yiuvKDs7W7Gxsfrggw90ww03aPXq1Xr11Vf15ptv2j0iANju9ddfD3puWZaqq6u1fPlyJSUl6a233rJpMmfgDIoDnTx5MnA/ntjY2MAHvb7//e/rkUcesXM0OFhxcbESEhI0duxYSVJeXp5eeOEF9evXT6+++mrgq59Ae5kwYULQc5fLpauuukrDhw/XkiVL7BnKQbjUvQOlpqbqk08+kST169dPv/3tbyVJGzdubHVTLKC95OfnKzIyUpL03nvvafny5Vq8eLESEhL0+OOP2zwdnOjcuXNBj5aWFvl8PpWUlKhPnz52j9fp8RaPAy1dulRhYWGaPXu2tm7dqrFjx6qlpUVnz55VYWGhHnvsMbtHhANFRUXp0KFDSk5O1pNPPqnq6mq98sorOnDggLKzs/X555/bPSIcqrm5WZWVlbr++usVHs4bD+2Ff9MO9Pd/Gx02bJgOHTqknTt36vrrr9fNN99s42Rwsu7du+vEiRNKTk7Wpk2bAsdpt27d1NjYaPN0cKKGhgY9+uijgQu2HT58WKmpqZo9e7a8Xq+eeuopmyfs3HiLB0pOTtbEiROJE9hq5MiRevDBB/Xggw/q8OHDgc+iHDhwQNddd529w8GR5s+fr7179+rdd98NukdUTk6O1q1bZ+NkzsAZFAdatmzZRbe7XC5169ZNN9xwg4YMGcLNsNCufvWrX+lf//VfVVVVpfXr16tXr16SpIqKCt1zzz02Twcn+t3vfqd169bp9ttvD7rCcb9+/fSXv/zFxsmcgc+gOFBKSoo+//xzNTQ0qGfPnrIsS1988YWioqLUvXt31dTUKDU1VVu3blVSUpLd4wKALaKiorR//36lpqYqJiZGe/bsUWpqqvbs2aMhQ4bI7/fbPWKnxls8DpSfn69bb71VH330kU6cOKGTJ0/q8OHDGjRokJ577jkdOXJEHo+Hb06g3X3xxRdasmSJHnzwQT300EMqLCzklwBsc+utt+qNN94IPD9/FuXFF1/kxqrtgDMoDnT99ddr/fr1+u53vxu0fdeuXbr77rv18ccfq7y8XHfffbeqq6vtGRKOs3PnTo0ePVqRkZG67bbbZFmWdu7cqcbGRm3atEm33HKL3SPCYcrLy3XHHXdo6tSpWrVqlaZPn64DBw7ovffeU1lZmTIzM+0esVPjDIoDVVdX6+zZs622nz17Vj6fT5Lk9XpVX1/f3qPBwR5//HGNHz9en3zyiTZs2KDS0lJVVlZq3Lhxys3NtXs8OFBWVpb+9Kc/qaGhQddff702bdqkxMREvffee8RJO+AMigONHTtWPp9PL730kgYMGCDpb2dPHnroIXk8Hv3+97/Xxo0b9ZOf/ET79u2zeVo4RWRkpHbt2qXvfOc7QdsPHjyogQMHqqGhwabJANiBMygOtHLlSsXHxyszM1Nut1tut1sDBw5UfHy8Vq5cKelv16TgUs5oT7GxsTpy5Eir7VVVVYqJibFhIjhVly5dFBYWdtkHF2y78jiD4mCHDh3S4cOHZVmWvvOd7+jGG2+0eyQ42OzZs1VaWqpf/vKXysrKksvl0rZt2/TEE0/o7rvvVlFRkd0jwiFee+21S+4rLy/X888/L8uyuIDgFUagOBiXb4ZJmpublZeXpxUrVgQ+I9W1a1c98sgjWrRokdxut80TwskOHTqk+fPna+PGjZo6dap++tOfKjk52e6xOjXe4nGghoYGTZs2TVFRUbrpppsCp9Vnz56tRYsW2TwdnKahoUEzZ85USkqKSkpKNGHCBL377rvatWuXTp48qaVLlxInsM2xY8f00EMPqX///jp79qx2796t4uJi4qQdECgONH/+fO3Zs4fLN8MITz/9tFatWqWxY8fqnnvu0TvvvKNly5apf//+ioqKsns8OJTf79eTTz6pG264QQcOHNAf/vAHbdy4Uenp6XaP5hic13cgLt8Mk2zYsEErV67U5MmTJUlTp07V9773PbW0tHC7Bdhi8eLF+sUvfiGPx6NXX31VP/jBD+weyZH4DIoDcflmmCQiIkKVlZW6+uqrA9siIyN1+PBhbrUAW3Tp0kWRkZHKycm5bCRv2LChHadyHs6gOND5yzfPmjVLEpdvhr1aWloUERERtC08PPyiFxME2sO9994bdHYZ9iBQHKigoEB33HGHDh48qLNnz+q5554Lunwz0J4sy9KPf/zjoA/Cfvnll3r44YcVHR0d2MbfVtFeVq1aZfcIEG/xONa+ffv0y1/+UhUVFTp37pxuueUWPfnkk8rIyLB7NDjM/fff/7XWvfzyy1d4EgAmIVAAAIBxeIvHQbp06fKV76u6XC7e+wcA2I5AcZDS0tJL7vv7yzcDAGA33uJxOC7fDAAwEVeSdSgu3wwAMBmB4jBcvhkA0BHwGRQH4fLNAICOgs+gOAiXbwYAdBScQXEQLt8MAOgoOIMCAACMw4dkAQCAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABjn/wEQZ/GqRn13rgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(df['LABEL'].value_counts())\n",
    "\n",
    "# Plot a bar chart of the value counts\n",
    "df['LABEL'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a94d8",
   "metadata": {},
   "source": [
    "### Now logistic regression model is implemented to classify the comments based on their text. Data are being splited into test and training then the model's performance is measured through accuracy rate and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e927931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Meghna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77500\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2))\n",
    "\n",
    "# Preprocess and vectorize the comments\n",
    "def preprocess_comment(comment):\n",
    "    # Do any preprocessing you want here\n",
    "    # ...\n",
    "    return comment\n",
    "\n",
    "# ...\n",
    "\n",
    "processed_comments = []\n",
    "for comment in df['COMMENTS']:\n",
    "    processed_comment = preprocess_comment(comment)\n",
    "    processed_comments.append(processed_comment)\n",
    "\n",
    "# Fit the vectorizer on the processed comments\n",
    "vectors = vectorizer.fit_transform(processed_comments)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, df['LABEL'], test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53cb4851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.91      0.83       112\n",
      "     Neutral       0.86      0.18      0.29        34\n",
      "    Positive       0.80      0.83      0.81        94\n",
      "\n",
      "    accuracy                           0.78       240\n",
      "   macro avg       0.80      0.64      0.64       240\n",
      "weighted avg       0.79      0.78      0.75       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204554f",
   "metadata": {},
   "source": [
    "This method is generally known as a BoW (Bag of Words) approach because it converts the text into a bag of words (i.e., a set of word counts) that are then used as features for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1826e",
   "metadata": {},
   "source": [
    "Now test comments are passed to check whether the model can predict the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfd20b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: ['Neutral']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the comment\n",
    "new_comment=\"This movie is average!\" # expected for this sentence's sentiment to be as neutral\n",
    "processed_comment = preprocess_comment(new_comment)\n",
    "\n",
    "# Vectorize the comment\n",
    "vectors = vectorizer.transform([processed_comment])\n",
    "\n",
    "# Use the model to predict the sentiment\n",
    "prediction = model.predict(vectors)\n",
    "\n",
    "print(\"The sentiment is:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9791ec3",
   "metadata": {},
   "source": [
    "Another classifier model called random forest has been implemented to train a sentiment analysis model.It is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26277f66",
   "metadata": {},
   "source": [
    "At first the functions for preprocess comment and predict sentiment are being defined to implement in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fad6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    # Do any preprocessing you want here\n",
    "    # ...\n",
    "    return comment\n",
    "\n",
    "\n",
    "def predict_sentiment(classifier, comment, vectorizer):\n",
    "    # Preprocess the comment\n",
    "    comment = preprocess_comment(comment)\n",
    "\n",
    "    # Convert the comment to a vector\n",
    "    comment_vector = vectorizer.transform([comment])\n",
    "\n",
    "    # Predict the sentiment of the comment\n",
    "    sentiment = classifier.predict(comment_vector)[0]\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aaeef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "['Positive' 'Negative' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Define the comments and labels\n",
    "comments = df['COMMENTS'].tolist()\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the comments in the training and test sets\n",
    "X_train = [preprocess_comment(comment) for comment in X_train]\n",
    "X_test = [preprocess_comment(comment) for comment in X_test]\n",
    "\n",
    "\n",
    "# Vectorize the comments in the training and test sets\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = classifier.score(X_test_vectors, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "test_comments = ['This movie is great!', 'This movie is bad', 'This movie is average'] # expected sentiments are positive, negative, neutral\n",
    "y_pred = classifier.predict(vectorizer.transform(test_comments))\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a52b0a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.79      0.84       112\n",
      "     Neutral       0.75      0.53      0.62        34\n",
      "    Positive       0.72      0.88      0.79        94\n",
      "\n",
      "    accuracy                           0.79       240\n",
      "   macro avg       0.79      0.74      0.75       240\n",
      "weighted avg       0.80      0.79      0.79       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283b436",
   "metadata": {},
   "source": [
    "### evaluation of results\n",
    "\n",
    "The first set of evaluation results show generally high precision and recall scores for all three categories of sentiment (negative, neutral, positive). The support values indicate that there are 112 negative, 34 neutral, and 94 positive examples in the test set.\n",
    "\n",
    "The second set of results show generally lower scores for all three categories of sentiment. The precision for negative and positive sentiment is higher than that of the first set of results, while the recall is lower. The neutral sentiment has a lower precision and recall. The support values indicate that there are 112 negative, 34 neutral and 94 positive examples in the test set.\n",
    "\n",
    "#### Comparison\n",
    "\n",
    "In general, precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive examples. A high precision means that the model is good at not labeling negative examples as positive, while a high recall means that the model is good at finding all the positive examples. In this case, the first set of results show generally better scores on both precision and recall, indicating that the model is making fewer false positive predictions and is able to find more actual positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47678b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
